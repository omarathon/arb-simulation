test.sh:

#!/bin/bash

set -e  # Exit script if any command fails

echo "🛑 Stopping any existing test containers..."
docker-compose -f docker-compose-test.yml down -v  # Stop test containers

echo "🚀 Starting test containers..."
docker-compose -f docker-compose-test.yml up -d --build  # Start test containers

# Wait for PostgreSQL
echo "⏳ Waiting for test database to be ready..."
until docker exec test_db pg_isready -U test_admin -d test_odds_db; do
  sleep 2
done
echo "✅ Test database is ready!"

# Wait for Redis
echo "⏳ Waiting for test Redis..."
until nc -z localhost 6380; do
  sleep 2
done
echo "✅ Test Redis is ready!"

# Wait for Scraper API
echo "⏳ Waiting for test Scraper API..."
until curl -s http://localhost:8002/docs > /dev/null; do
  sleep 2
done
echo "✅ Test Scraper API is ready!"

# echo "🧪 Running tests..."
# docker exec test_scraper pytest --disable-warnings || exit 1

# Stop test containers after tests
echo "🛑 Stopping test containers..."
# docker-compose -f docker-compose-test.yml down -v --remove-orphans


--------------------------------------------------------------------------------
docker-compose-test.yml:

version: '3.8'

services:
  base_image:
    build:
      context: .
      dockerfile: backend/Dockerfile.base
    image: smarkets-base-image

  test_db:
    image: postgres:15
    container_name: test_db
    restart: always
    environment:
      POSTGRES_DB: test_odds_db
      POSTGRES_USER: test_admin
      POSTGRES_PASSWORD: test_password
    ports:
      - "5433:5432"  # Use a different port than production
    networks:
      - test_network

  test_redis:
    image: redis:latest
    container_name: test_redis
    restart: always
    ports:
      - "6380:6379"  # Use a different port than production
    networks:
      - test_network

  test_scraper:
    build:
      context: .
      dockerfile: backend/scraper/Dockerfile
    container_name: test_scraper
    restart: always
    depends_on:
      - base_image
      - test_db
      - test_redis
    environment:
      POSTGRES_DB: test_odds_db
      POSTGRES_USER: test_admin
      POSTGRES_PASSWORD: test_password
      POSTGRES_HOST: test_db
      POSTGRES_PORT: 5432
      REDIS_HOST: test_redis
      REDIS_PORT: 6379
      ODDS_PUBLISH_INTERVAL: 1
    ports:
      - "8002:8001"
    volumes:
      - ./backend:/app/backend
    networks:
      - test_network

  test_runner:
    build:
      context: .
      dockerfile: backend/scraper/Dockerfile.test
    container_name: test_runner
    depends_on:
      - base_image
      - test_scraper
      - test_db
      - test_redis
    environment:
      POSTGRES_DB: test_odds_db
      POSTGRES_USER: test_admin
      POSTGRES_PASSWORD: test_password
      POSTGRES_HOST: test_db
      POSTGRES_PORT: 5432
      REDIS_HOST: test_redis
      REDIS_PORT: 6379
      ODDS_PUBLISH_INTERVAL: 1
    networks:
      - test_network

networks:
  test_network:

--------------------------------------------------------------------------------
requirements.txt:

annotated-types==0.7.0
anyio==4.8.0
async-timeout==5.0.1
asyncio==3.4.3
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
dotenv==0.9.9
exceptiongroup==1.2.2
fastapi==0.115.11
gevent==24.11.1
greenlet==3.1.1
h11==0.14.0
httpcore==1.0.7
httpx==0.28.1
idna==3.10
iniconfig==2.0.0
packaging==24.2
pluggy==1.5.0
psycopg2==2.9.10
pydantic==2.10.6
pydantic_core==2.27.2
pytest==8.3.4
pytest-timeout==2.3.1
python-dotenv==1.0.1
redis==5.2.1
requests==2.32.3
sniffio==1.3.1
SQLAlchemy==2.0.38
starlette==0.46.0
tomli==2.2.1
typing_extensions==4.12.2
urllib3==2.3.0
uvicorn[standard]==0.34.0
websockets==12.0
websocket-client==1.8.0
zope.event==5.0
zope.interface==7.2


--------------------------------------------------------------------------------
docker-compose.yml:

version: '3.8'

services:
  db:
    image: postgres:15
    container_name: db
    restart: always
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  redis:
    image: redis:latest
    container_name: redis
    restart: always
    ports:
      - "6379:6379"

  scraper:
    build:
      context: .  # Build from project root, ensuring all backend files exist
      dockerfile: backend/scraper/Dockerfile
    container_name: scraper
    restart: always
    depends_on:
      - db
      - redis
    env_file:
      - .env
    ports:
      - "8001:8000"
    volumes:
      - ./backend:/app/backend  # Mount entire backend for proper imports


  # arb_detector:
  #   build: ./backend/arb_detector
  #   container_name: arb_detector
  #   restart: always
  #   depends_on:
  #     - db
  #     - redis
  #   env_file:
  #     - .env
  #   ports:
  #     - "8002:8000"  # Arbitrage Detector runs on port 8002
  #   volumes:
  #     - ./backend/arb_detector/src:/app
  #   command: uvicorn arb_detector.src.main:app --host 0.0.0.0 --port 8000 --reload

  # bet_executor:
  #   build: ./backend/bet_executor
  #   container_name: bet_executor
  #   restart: always
  #   depends_on:
  #     - db
  #     - redis
  #   env_file:
  #     - .env
  #   ports:
  #     - "8003:8000"  # Bet Executor runs on port 8003
  #   volumes:
  #     - ./backend/bet_executor/src:/app
  #   command: uvicorn bet_executor.src.main:app --host 0.0.0.0 --port 8000 --reload

  # frontend:
  #   build: ./frontend
  #   container_name: frontend
  #   restart: always
  #   depends_on:
  #     - scraper
  #     - arb_detector
  #     - bet_executor
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - ./frontend:/app
  #   command: npm start

volumes:
  pgdata:


--------------------------------------------------------------------------------
.dockerignore:

__pycache__/
*.pyc
*.pyo
*.pyd
venv/
.env

--------------------------------------------------------------------------------
README.md:

# smarkets-take-home

--------------------------------------------------------------------------------
pytest.ini:

[pytest]
pythonpath = .

--------------------------------------------------------------------------------
backend/Dockerfile.base:

FROM python:3.10
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

--------------------------------------------------------------------------------
backend/__init__.py:



--------------------------------------------------------------------------------
backend/requirements.txt:

annotated-types==0.7.0
anyio==4.8.0
async-timeout==5.0.1
asyncio==3.4.3
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
dotenv==0.9.9
exceptiongroup==1.2.2
fastapi==0.115.11
gevent==24.11.1
greenlet==3.1.1
h11==0.14.0
httpcore==1.0.7
httpx==0.28.1
idna==3.10
iniconfig==2.0.0
packaging==24.2
pluggy==1.5.0
psycopg2==2.9.10
pydantic==2.10.6
pydantic_core==2.27.2
pytest==8.3.4
pytest-timeout==2.3.1
python-dotenv==1.0.1
redis==5.2.1
requests==2.32.3
sniffio==1.3.1
SQLAlchemy==2.0.38
starlette==0.46.0
tomli==2.2.1
typing_extensions==4.12.2
urllib3==2.3.0
uvicorn[standard]==0.34.0
websockets==12.0
websocket-client==1.8.0
zope.event==5.0
zope.interface==7.2


--------------------------------------------------------------------------------
backend/shared/__init__.py:



--------------------------------------------------------------------------------
backend/shared/utils.py:



--------------------------------------------------------------------------------
backend/shared/models.py:

from sqlalchemy import Column, Integer, String, Float, Boolean, TIMESTAMP
from sqlalchemy.orm import declarative_base
import datetime

Base = declarative_base()

class Odds(Base):
    __tablename__ = "odds"

    id = Column(Integer, primary_key=True, index=True)
    match = Column(String, nullable=False)
    bookmaker = Column(String, nullable=False)
    home_win = Column(Float, nullable=False)
    draw = Column(Float, nullable=False)
    away_win = Column(Float, nullable=False)
    actionable = Column(Boolean, default=True)
    created_at = Column(TIMESTAMP, default=datetime.datetime.now(datetime.timezone.utc), nullable=False)


--------------------------------------------------------------------------------
backend/shared/config.py:

import os

class SharedConfig:
    DB_HOST = os.getenv("POSTGRES_HOST", "db")
    DB_NAME = os.getenv("POSTGRES_DB", "odds_db")
    DB_USER = os.getenv("POSTGRES_USER", "admin")
    DB_PASSWORD = os.getenv("POSTGRES_PASSWORD", "password")
    DB_PORT = int(os.getenv("POSTGRES_PORT", 5432))
    REDIS_HOST = os.getenv("REDIS_HOST", "redis")
    REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

shared_config = SharedConfig()


--------------------------------------------------------------------------------
backend/shared/database.py:

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from backend.shared.config import shared_config
import os

DATABASE_URL = f"postgresql://{shared_config.DB_USER}:{shared_config.DB_PASSWORD}@{shared_config.DB_HOST}:{shared_config.DB_PORT}/{shared_config.DB_NAME}"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def init_db():
    from backend.shared.models import Base
    Base.metadata.create_all(bind=engine)

def get_db():
    """Dependency for getting a database session."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


--------------------------------------------------------------------------------
backend/shared/__pycache__/database.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0x8c in position 9: invalid start byte]

--------------------------------------------------------------------------------
backend/shared/__pycache__/__init__.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xe2 in position 9: invalid continuation byte]

--------------------------------------------------------------------------------
backend/shared/__pycache__/config.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xc1 in position 8: invalid start byte]

--------------------------------------------------------------------------------
backend/shared/__pycache__/models.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xc4 in position 10: invalid continuation byte]

--------------------------------------------------------------------------------
backend/__pycache__/__init__.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xe2 in position 9: invalid continuation byte]

--------------------------------------------------------------------------------
backend/bet_executor/requirements.txt:



--------------------------------------------------------------------------------
backend/bet_executor/Dockerfile:



--------------------------------------------------------------------------------
backend/bet_executor/src/config.py:

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class BetExecutorConfig:
    REDIS_HOST = os.getenv("REDIS_HOST", "redis")
    REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
    EXECUTION_DELAY = int(os.getenv("EXECUTION_DELAY", 2))  # Simulated execution delay in seconds

bet_executor_config = BetExecutorConfig()


--------------------------------------------------------------------------------
backend/scraper/__init__.py:



--------------------------------------------------------------------------------
backend/scraper/Dockerfile:

FROM smarkets-base-image
WORKDIR /app
ENV PYTHONPATH=/app
EXPOSE 8001
CMD ["uvicorn", "backend.scraper.src.main:app", "--host", "0.0.0.0", "--port", "8001"]


--------------------------------------------------------------------------------
backend/scraper/Dockerfile.test:

FROM smarkets-base-image
WORKDIR /app
COPY backend /app/backend
COPY backend/scraper/tests /app/backend/scraper/tests
ENV PYTHONPATH=/app
ENTRYPOINT pytest

--------------------------------------------------------------------------------
backend/scraper/src/odds_publisher.py:

import redis
import json
import asyncio
import random
from sqlalchemy.orm import Session
from backend.scraper.src.config import ScraperConfig
from backend.shared.database import get_db
from backend.shared.models import Odds

redis_client = redis.Redis(host=ScraperConfig.REDIS_HOST, port=ScraperConfig.REDIS_PORT, db=0)

def store_odds(db: Session, match: str, bookmaker: str, home_win: float, draw: float, away_win: float):
    """Stores odds in the database."""
    new_odds = Odds(
        match=match,
        bookmaker=bookmaker,
        home_win=home_win,
        draw=draw,
        away_win=away_win
    )
    db.add(new_odds)
    db.commit()

async def publish_odds_loop():
    """Generates and publishes mock odds to Redis periodically."""
    matches = ["Man Utd vs Chelsea", "Liverpool vs Arsenal", "Barcelona vs Real Madrid"]
    bookmakers = ["Bet365", "Smarkets"]

    while True:
        db = next(get_db())
        for match in matches:
            for bookmaker in bookmakers:
                odds_data = {
                    "match": match,
                    "bookmaker": bookmaker,
                    "odds": {
                        "home_win": round(random.uniform(1.8, 2.5), 2),
                        "draw": round(random.uniform(3.0, 4.0), 2),
                        "away_win": round(random.uniform(1.8, 2.5), 2)
                    }
                }
                redis_client.publish("odds_update", json.dumps(odds_data))
                
                # Store in PostgreSQL
                store_odds(db, match, bookmaker, 
                           odds_data["odds"]["home_win"], 
                           odds_data["odds"]["draw"], 
                           odds_data["odds"]["away_win"])
        
        await asyncio.sleep(ScraperConfig.ODDS_PUBLISH_INTERVAL)

def start_odds_publisher():
    """Starts the odds publishing loop in a background thread."""
    import threading
    threading.Thread(target=asyncio.run, args=(publish_odds_loop(),)).start()


--------------------------------------------------------------------------------
backend/scraper/src/__init__.py:



--------------------------------------------------------------------------------
backend/scraper/src/main.py:

from fastapi import FastAPI, WebSocket
from backend.scraper.src.websocket_handler import websocket_endpoint
from backend.scraper.src.redis_listener import start_redis_listener
from backend.scraper.src.odds_publisher import start_odds_publisher
from backend.shared.database import init_db

app = FastAPI(
    title="Scraper API",
    description="API for scraping and publishing odds",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

@app.get("/health", tags=["System"])
async def health_check():
    return {"status": "OK"}

init_db()

app.add_api_websocket_route("/ws", websocket_endpoint)

# Start backend processes
start_redis_listener()
start_odds_publisher()


--------------------------------------------------------------------------------
backend/scraper/src/config.py:

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class ScraperConfig:
    REDIS_HOST = os.getenv("REDIS_HOST", "redis")
    REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
    ODDS_PUBLISH_INTERVAL = int(os.getenv("ODDS_PUBLISH_INTERVAL", 5))

scraper_config = ScraperConfig()


--------------------------------------------------------------------------------
backend/scraper/src/redis_listener.py:

import redis
import json
import asyncio
from backend.scraper.src.websocket_handler import broadcast_message
from backend.scraper.src.config import ScraperConfig

redis_client = redis.Redis(host=ScraperConfig.REDIS_HOST, port=ScraperConfig.REDIS_PORT, db=0)

async def redis_listener():
    """Listens for Redis messages and broadcasts them to WebSocket clients."""
    pubsub = redis_client.pubsub()
    pubsub.subscribe("odds_update")

    for message in pubsub.listen():
        if message["type"] == "message":
            data = json.loads(message["data"])
            await broadcast_message(data)

def start_redis_listener():
    """Starts the Redis listener in a background thread."""
    import threading
    threading.Thread(target=asyncio.run, args=(redis_listener(),)).start()


--------------------------------------------------------------------------------
backend/scraper/src/websocket_handler.py:

from fastapi import WebSocket
import asyncio

connected_clients = []

async def websocket_endpoint(websocket: WebSocket):
    """Handles WebSocket connections for real-time updates."""
    await websocket.accept()
    connected_clients.append(websocket)
    try:
        while True:
            await asyncio.sleep(1)
    except Exception:
        connected_clients.remove(websocket)

async def broadcast_message(data):
    """Send data to all connected WebSocket clients."""
    for ws in connected_clients:
        try:
            await ws.send_json(data)
        except Exception:
            connected_clients.remove(ws)


--------------------------------------------------------------------------------
backend/scraper/src/__pycache__/websocket_handler.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0x88 in position 8: invalid start byte]

--------------------------------------------------------------------------------
backend/scraper/src/__pycache__/__init__.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xe2 in position 9: invalid continuation byte]

--------------------------------------------------------------------------------
backend/scraper/src/__pycache__/odds_publisher.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0x8c in position 9: invalid start byte]

--------------------------------------------------------------------------------
backend/scraper/src/__pycache__/redis_listener.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0x87 in position 8: invalid start byte]

--------------------------------------------------------------------------------
backend/scraper/src/__pycache__/config.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xdd in position 8: invalid continuation byte]

--------------------------------------------------------------------------------
backend/scraper/src/__pycache__/main.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0x8c in position 9: invalid start byte]

--------------------------------------------------------------------------------
backend/scraper/__pycache__/__init__.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xe2 in position 9: invalid continuation byte]

--------------------------------------------------------------------------------
backend/scraper/tests/test_websocket.py:

import pytest
import time
from backend.scraper.src.config import scraper_config

@pytest.mark.timeout(scraper_config.ODDS_PUBLISH_INTERVAL + 10)
def test_websocket_receives_odds():
    """Test if WebSocket sends live odds to clients."""
    
    ws_url = "ws://test_scraper:8001/ws"
    
    import websocket
    
    ws = websocket.create_connection(ws_url)

    try:
        ws.send("subscribe")
        time.sleep(scraper_config.ODDS_PUBLISH_INTERVAL + 2)
        response = ws.recv()
        assert response is not None, "No data received from WebSocket!"
        assert "match" in response, f"Unexpected response: {response}"
    finally:
        ws.close()


--------------------------------------------------------------------------------
backend/scraper/tests/test_scraper_database.py:

import time
from sqlalchemy.orm import sessionmaker
from backend.shared.database import engine
from backend.shared.models import Odds
from backend.scraper.src.config import scraper_config

SessionTesting = sessionmaker(bind=engine)

def test_scraper_stores_odds_in_database():
    """Test that the scraper correctly stores odds in the test PostgreSQL database."""

    session = SessionTesting()

    # Wait for odds to be published
    time.sleep(scraper_config.ODDS_PUBLISH_INTERVAL + 2)

    stored_odds = session.query(Odds).all()
    session.close()

    assert stored_odds, "No odds were stored in the database!"
    
    first_odd = stored_odds[0]
    assert first_odd.match, "Missing 'match' field"
    assert first_odd.bookmaker, "Missing 'bookmaker' field"
    assert first_odd.home_win is not None, "Missing 'home_win' field"
    assert first_odd.draw is not None, "Missing 'draw' field"
    assert first_odd.away_win is not None, "Missing 'away_win' field"


--------------------------------------------------------------------------------
backend/scraper/tests/__init__.py:



--------------------------------------------------------------------------------
backend/scraper/tests/test_scraper_redis.py:

import redis
import json
import time
from backend.shared.config import shared_config
from backend.scraper.src.config import scraper_config

redis_client = redis.Redis(host=shared_config.REDIS_HOST, port=shared_config.REDIS_PORT, db=0)

def test_scraper_publishes_odds_to_redis():
    """Test that the scraper correctly publishes odds to Redis."""

    pubsub = redis_client.pubsub()
    pubsub.subscribe("odds_update")

    timeout = scraper_config.ODDS_PUBLISH_INTERVAL + 2
    start_time = time.time()

    message = None
    while time.time() - start_time < timeout:
        message = pubsub.get_message(ignore_subscribe_messages=True)
        if message:
            break

    assert message is not None, "No odds were published to Redis!"
    
    odds_data = json.loads(message["data"])
    assert "match" in odds_data, "Missing 'match' field in Redis odds!"
    assert "bookmaker" in odds_data, "Missing 'bookmaker' field in Redis odds!"


--------------------------------------------------------------------------------
backend/scraper/tests/__pycache__/__init__.cpython-310.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xd4 in position 9: invalid continuation byte]

--------------------------------------------------------------------------------
backend/scraper/tests/__pycache__/test_websocket.cpython-310-pytest-8.3.4.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xc4 in position 10: invalid continuation byte]

--------------------------------------------------------------------------------
backend/scraper/tests/__pycache__/conftest.cpython-310-pytest-8.3.4.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xed in position 9: invalid continuation byte]

--------------------------------------------------------------------------------
backend/scraper/tests/__pycache__/test_scraper_database.cpython-310-pytest-8.3.4.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xfc in position 8: invalid start byte]

--------------------------------------------------------------------------------
backend/scraper/tests/__pycache__/test_scraper_redis.cpython-310-pytest-8.3.4.pyc:

[ERROR READING FILE: 'utf-8' codec can't decode byte 0xfa in position 8: invalid start byte]

--------------------------------------------------------------------------------
backend/arb_detector/requirements.txt:



--------------------------------------------------------------------------------
backend/arb_detector/Dockerfile:



--------------------------------------------------------------------------------
backend/arb_detector/src/config.py:

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class ArbDetectorConfig:
    REDIS_HOST = os.getenv("REDIS_HOST", "redis")
    REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
    ARB_THRESHOLD = float(os.getenv("ARB_THRESHOLD", 0.02))  # Arbitrage detection threshold

arb_detector_config = ArbDetectorConfig()


--------------------------------------------------------------------------------
.pytest_cache/CACHEDIR.TAG:

Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html


--------------------------------------------------------------------------------
.pytest_cache/README.md:

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


--------------------------------------------------------------------------------
.pytest_cache/v/cache/stepwise:

[]

--------------------------------------------------------------------------------
.pytest_cache/v/cache/nodeids:

[]

--------------------------------------------------------------------------------
.pytest_cache/v/cache/lastfailed:

{
  "backend/scraper/tests/test_websocket.py": true
}

--------------------------------------------------------------------------------
